{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "\n",
        "# Replace with your actual API key\n",
        "wandb_api_key = \"30f827999aedd60e84d5c0815f557a8ba3462638\"\n",
        "\n",
        "# Login to wandb\n",
        "wandb.login(key=wandb_api_key)\n"
      ],
      "metadata": {
        "id": "u-8Sqa8HAmlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class KnowledgePreservationLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.7):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.kl_div = nn.KLDivLoss(reduction='batchmean')\n",
        "\n",
        "    def forward(self, student_logits, teacher_logits, math_mask):\n",
        "        # math_mask identifies mathematical tokens\n",
        "        base_loss = self.kl_div(student_logits, teacher_logits)\n",
        "        math_loss = self.kl_div(student_logits[math_mask],\n",
        "                              teacher_logits[math_mask])\n",
        "        return self.alpha * math_loss + (1 - self.alpha) * base_loss\n"
      ],
      "metadata": {
        "id": "DhlNawe-__Oy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MobileLongformer(Module):\n",
        "    def __init__(self, base_model):\n",
        "        self.window_size = 65536\n",
        "        self.overlap = 512\n",
        "        self.kv_cache = CompressedCache(num_bits=4, ecc_bits=2)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        # Process in chunks with overlapping regions\n",
        "        outputs = []\n",
        "        for i in range(0, len(input_ids), self.window_size):\n",
        "            chunk = input_ids[i:i+self.window_size+self.overlap]\n",
        "            chunk_out = self.base_model(chunk, kv_cache=self.kv_cache)\n",
        "            outputs.append(chunk_out[:-self.overlap])\n",
        "        return torch.cat(outputs)\n"
      ],
      "metadata": {
        "id": "BeVHSQJIADMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "math_data = load_dataset(\"qwen/math-corpus-v2\", split=\"train\")\n",
        "blockchain_data = load_dataset(\"web3/technical-docs\", split=\"train\")\n",
        "\n",
        "def filter_non_technical(example):\n",
        "    return example[\"category\"] in (\"math\", \"blockchain\")\n",
        "\n",
        "train_data = concatenate_datasets([math_data, blockchain_data]).filter(filter_non_technical)\n"
      ],
      "metadata": {
        "id": "YJ2gnunJAECU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import wandb\n",
        "\n",
        "# Initialize Weights & Biases\n",
        "wandb.init(project=\"qwen-distill\",\n",
        "           config={\n",
        "               \"base_model\": \"qwen/Qwen2.5-1.5B\",\n",
        "               \"target_size\": \"600MB\",\n",
        "               \"max_length\": 200_000\n",
        "           })\n",
        "\n",
        "# Load teacher model (full Qwen 2.5 7B)\n",
        "teacher = AutoModelForCausalLM.from_pretrained(\"qwen/Qwen2.5-7B\",\n",
        "                                             device_map=\"auto\")\n",
        "\n",
        "# Initialize student with pruned architecture\n",
        "student = create_pruned_model(teacher, keep_layers=[4,5,6,7,8,9,10,11,12])\n",
        "\n",
        "# Configure mobile-optimized trainer\n",
        "from transformers import MobileTrainingArguments\n",
        "\n",
        "args = MobileTrainingArguments(\n",
        "    output_dir=\"qwen-math-distilled\",\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=8,\n",
        "    learning_rate=3e-5,\n",
        "    weight_decay=0.01,\n",
        "    fp16=True,\n",
        "    optim=\"adafactor\",\n",
        "    report_to=\"wandb\",\n",
        "    mobile_optimizations=True  # Enables ARM NEON instructions\n",
        ")\n"
      ],
      "metadata": {
        "id": "douFjI7AAIQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi\n",
        "\n",
        "api = HfApi(token=\"f_VfSYPuYfShOGDpZpMVjEGwVIfXrgWVwnaJ\")\n",
        "api.create_repo(repo_id=\"Ayansk11/qwen-math-distilled\", private=True)\n",
        "\n",
        "student.push_to_hub(\n",
        "    \"username/qwen-math-distilled\",\n",
        "    commit_message=\"Initial distilled release\",\n",
        "    max_shard_size=\"200MB\"\n",
        ")\n",
        "\n",
        "# Generate model card\n",
        "with open(\"README.md\", \"w\") as f:\n",
        "    f.write(f\"\"\"---\n",
        "license: apache-2.0\n",
        "base_model: qwen/Qwen2.5-1.5B\n",
        "tags:\n",
        "- mathematics\n",
        "- blockchain\n",
        "- mobile\n",
        "---\n",
        "\n",
        "# Distilled Qwen 2.5 Math/Blockchain Specialist\n",
        "\"\"\")\n",
        "\n",
        "api.upload_file(\n",
        "    repo_id=\"username/qwen-math-distilled\",\n",
        "    path_in_repo=\"README.md\",\n",
        "    path_or_fileobj=\"README.md\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "NKHtMB_lAQIx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}